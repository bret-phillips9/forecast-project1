---
title: "forecast-project"
author: "Bret Phillips"
date: "2025-03-15"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Load Required Libraries

```{r}
library(forecast)
library(lubridate)
library(dplyr)
library(ggplot2)
```

## Import Data

```{r}
time_raw <- read.csv("./dataset_for_technical_assessment.csv")
summary(time_raw)
```

# Data Cleaning and Preprocessing

Above, we see 74 records with NAs. There is no choice about those missing y values; they must be deleted.  Any remaining missing on the X's can be mean-imputed, which appears to affect only one additional record.

## Data Cleaning

```{r}
time_clean <- time_raw |> 
     # delete missing y's, impute missing x's
     filter(!is.na(y1)) |> 
     filter(!is.na(y2)) |> 
     mutate(x1 = ifelse(is.na(x1), mean(x1, na.rm = TRUE), x1)) |> 
     mutate(x2 = ifelse(is.na(x2), mean(x2, na.rm = TRUE), x2)) |> 
     mutate(x3 = ifelse(is.na(x3), mean(x3, na.rm = TRUE), x3)) |> 
     mutate(x4 = ifelse(is.na(x4), mean(x4, na.rm = TRUE), x4)) |> 
     mutate(x5 = ifelse(is.na(x5), mean(x5, na.rm = TRUE), x5)) |> 
     mutate(x6 = ifelse(is.na(x6), mean(x6, na.rm = TRUE), x6)) |> 
     mutate(x7 = ifelse(is.na(x7), mean(x7, na.rm = TRUE), x7)) |> 
     mutate(x8 = ifelse(is.na(x8), mean(x8, na.rm = TRUE), x8)) |> 
     as.data.frame()  
     
summary(time_clean)


```

## Time Intelligence

Our date-time variable imported as character. This is not useful. We use lubridate to convert it to an actual date-time variable called timestamp. The original character variable is discarded.

```{r}
time_clean <- time_clean |> 
     mutate(timestamp = ymd_hms(Description)) |> 
     mutate(day = ymd(substr(Description, start = 1, stop=10))) |> 
     mutate(hour = paste("2016-09-30 ", format(timestamp, "%H:%M:%S"))) |> 
     mutate(from_start = timestamp - timestamp[1]) |> 
     select(-Description)

summary(time_clean$timestamp)


```

## Target Variable Calculation

Observe the plot of y1 and y2 over time.

```{r}
second_series <- tibble(timestamp=time_clean$timestamp, y1=time_clean$y2)

ggplot(time_clean, aes(timestamp, y1))+
     geom_point(data=time_clean, color="blue")+
     geom_point(data=second_series, color="red")
```

The target variables have a nearly perfectly complementary relationship.  Therefore, it is reasonable to fuse y1 and y2 into a single target variable.  However, there are instances where both targets have nonzero values.  We shall add a new regressor, both_nonzero, to account for this event.

```{r}
time_clean <- time_clean |>
     mutate(y_joint = y1 + y2) |> 
     mutate(y1_zero = ifelse(y1 == 0, 1, 0)) |> 
     mutate(y2_zero = ifelse(y2 == 0, 1, 0))
            
summary(time_clean)
```


## Train-Validation Split

The original exercise calls for using whatever models are dervied on the training data to the next two weeks of data.  However, this would require X data from those two weeks and these data were not provided.  Therefore, we will hold back the last two weeks of data from our dataset (4032 obs) as our validation sample to satisfy this portion of the assignment.

```{r}
time_val <- tail(time_clean, n = 4032)

time_train <- setdiff(time_clean, time_val)
```


# Exploratory Data Analysis

There is so much data here, it may be difficult to get a good feel for trends. Thus, I separated the training data into day and time dimensions so the data may be summarized each way to get a better sense of it, before a full ARIMA model is attempted. Time data alone cannot be used as an index for ARIMA models in R, so I append the date 2016-09-30 to all the time values in the hourly data set.

```{r}
daily <- time_train |> 
     group_by(day) |> 
     summarize(y_joint = mean(y_joint),
               x1 = mean(x1),
               x2 = mean(x2),
               x3 = mean(x3),
               x4 = mean(x4),
               x5 = mean(x5),
               x6 = mean(x6),
               x7 = mean(x7),
               x8 = mean(x8),
               y1_zero = mean(y1_zero),
               y2_zero = mean(y2_zero)) 

hourly <- time_train |> 
     group_by(hour) |> 
     summarize(y_joint = mean(y_joint),
               y2 = mean(y2),
               x1 = mean(x1),
               x2 = mean(x2),
               x3 = mean(x3),
               x4 = mean(x4),
               x5 = mean(x5),
               x6 = mean(x6),
               x7 = mean(x7),
               x8 = mean(x8),
               y1_zero = mean(y1_zero),
               y2_zero = mean(y2_zero))  


```



## Y1

### Raw Data

We begin with plotting the Y1 values over time.

```{r}
ggplot(daily, aes(x=day, y=y_joint)) +
     geom_point()

ggplot(hourly, aes(x=hour, y=y_joint)) +
     geom_point()


```

### ACF and PACF

Now we plot the autocorrelation and partial autocorrelation functions, to get a sense of the error structure of y_joint.

```{r}
y_day <- ts(daily$y_joint, daily$day)

Acf(x = y_day, main = "Autocorrelation Function for Y_joint by Day")
Pacf(x = y_day, main = "Partial Autocorrelation Function for Y_joint by Day")

y_hour <- ts(hourly$y_joint, as.POSIXct(hourly$hour))

Acf(x = y_hour, main = "Autocorrelation Function for Y_joint by Hour")
Pacf(x = y_hour, main = "Partial Autocorrelation Function for Y_joint by Hour")
```

The gradual descent of the ACFs show that the data are non-stationary and thus will require differencing.  The PACFs for daily data show a spike at lag 1 (at least), indicating the need for AR terms.

# Forecasting

Prep the time series object using the ts function from base R.  Also, auto.arima requires that regressors be passed as a matrix object, so we prepare that matrix too.

```{r}
y_train <- ts(time_train$y_joint, time_train$timestamp)

# also prep predictor matrix, which applies to both models
x_train <- as.matrix(time_train[, c(3:10,16:17)])

# and, of course, we must also do this for the validation data
y_val <- ts(time_val$y_joint, time_val$timestamp)

# also prep predictor matrix for validation for forecasting
x_val <- as.matrix(time_val[, c(3:10,16:17)])

```

## Modeling - Y_joint

### Null Model

To understand the effect of the predictors, we begin with a null model that does not contain predictors.

```{r}
y_null <- auto.arima(y = y_train)
summary(y_null)
```

The auto.arima algorithm included 4 AR and 5 MA terms, which is not surprising given the results of the ACF and PACF plots.

Now we add the regressors into the ARIMA model.

### Full Model

```{r}
y_fit <- auto.arima(y = y_train, xreg = x_train)
summary(y_fit)
```
Interestingly, the inclusion of regressors replaced two of the MA terms.

Because the null and full models are of different (p, d, q) orders, a direct comparison of AICs or BICs is not informative.  However, we can see that RMSE of the full model is about .0013 lower than that of the null model, indicating that some useful variance is captured by the predictors.

### Residuals - Full Model

Below we plot the residuals of the full (4, 1, 3) ARIMA model.

```{r}
tsdisplay(residuals(y_fit), lag.max = 40, main = "Y_joint (4,1,3) Full Model Residuals - Training Data")
```

The plots suggest that the residuals are normally distributed.

### Forecast for Next 14 Days

As noted above, to complete the next portion of the assignment, we will refer to our validation data.  Specifically, the x_val data, which contain our new regressor data.

```{r}
y_fcast <- forecast(y_fit, xreg = x_val)

plot(y_fcast)
```

Forecasting on the validation data looks pretty reasonable.

Let's examine the distribution of residuals to see what the actual performance of our model was.

```{r}
time_val$resid <- time_val$y_joint - y_fcast$mean

summary(time_val$resid)
boxplot(time_val$resid)
```

Residuals are normally distributed with a near-zero mean, though there are some extreme outliers.


# Conclusion

It would be better if we had substantive context (domain knowledge) to understand what these data represent.  Still, it seems as though we got fairly good prediction on this model, despite the presence of some anomalous data. 

Perhaps tuning the (p, d, q) hyperparameters would improve the model fit. The auto.arima function is supposed to find the optimal AIC/BIC, but perhaps it can get stuck in a local minimum.

Another idea would be to build an LSTM model to see if the fit can be improved.